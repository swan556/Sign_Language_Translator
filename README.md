# Sign Language Detection Using Deep Learning

## ğŸ“Œ Project Overview
This project is a *real-time Sign Language Detection System* that utilizes *computer vision and deep learning* to recognize hand gestures and predict corresponding sign language alphabets. The system processes a *live video feed, extracts hand landmarks, and classifies them using a **trained neural network model*.

## ğŸš€ Features
- *Real-Time Gesture Recognition* using a webcam.
- *Deep Learning Model* trained on hand landmarks for sign language classification.
- *Flask Backend* for handling video streaming and model inference.
- *React Frontend* with a sleek and interactive UI.
- *Smooth User Experience* with real-time predictions displayed dynamically.

## ğŸ— Model Architecture
The *sign language classifier* is a *3-layered feed-forward neural network* trained on hand landmark data extracted using *MediaPipe Hands*.

### *ğŸ”¢ Model Details*
- *Input:* 63-dimensional vector (21 hand landmarks Ã— 3 coordinates each: x, y, z)
- *Hidden Layers:*
  - *Layer 1:* 128 neurons, ReLU activation
  - *Layer 2:* 64 neurons, ReLU activation
  - *Output Layer:* 25 neurons (for 25 alphabets except 'J' and 'Z'), Softmax activation
- *Loss Function:* Sparse Categorical Crossentropy
- *Optimizer:* Adam Optimizer
- *Dataset:* 1500 samples per alphabet (manually recorded hand landmarks)
- *Batch Size:* 32
- *Epochs:* 50

## ğŸ— Tech Stack
### *Backend:*
- Flask
- OpenCV
- TensorFlow/Keras
- MediaPipe

### *Frontend:*
- React
- Tailwind CSS
- Lucide Icons

### *Deployment:*
- *Frontend:* Netlify
- *Backend:* Render

## ğŸ“‚ Project Structure

ğŸ“ sign-language-detection
â”‚â”€â”€ ğŸ“‚ backend
â”‚   â”‚â”€â”€ app.py  # Flask server
â”‚   â”‚â”€â”€ pretrained_model.h5  # Trained model
â”‚   â”‚â”€â”€ requirements.txt  # Dependencies
â”‚
â”‚â”€â”€ ğŸ“‚ frontend
â”‚   â”‚â”€â”€ src
â”‚   â”‚   â”‚â”€â”€ App.tsx  # Main React file
â”‚   â”‚   â”‚â”€â”€ components/  # UI Components
â”‚   â”‚â”€â”€ package.json  # React dependencies
â”‚
â”‚â”€â”€ README.md  # Project Documentation


## ğŸ¯ How It Works
1ï¸âƒ£ *User starts the camera*, and the system captures real-time hand gestures.
2ï¸âƒ£ *MediaPipe Hands* detects hand landmarks and sends them to the model.
3ï¸âƒ£ *Trained Neural Network* processes the landmarks and predicts the corresponding letter.
4ï¸âƒ£ *The predicted sign is displayed* on the UI in real time.

## ğŸ”§ Installation & Setup
### *1ï¸âƒ£ Clone the repository*
bash
git clone https://github.com/your-repo/sign-language-detection.git
cd sign-language-detection


### *2ï¸âƒ£ Setup the backend*
bash
cd backend
pip install -r requirements.txt
python app.py


### *3ï¸âƒ£ Setup the frontend*
bash
cd frontend
npm install
npm run dev


### *4ï¸âƒ£ Open in browser*

[http://localhost:5173/](https://glittery-tanuki-240cf1.netlify.app/)


## ğŸ“Œ Future Enhancements
âœ… Increase dataset size for better accuracy.  
âœ… Train with a *CNN-based architecture* for improved performance.  
âœ… Expand the model to support numbers and words.  

## ğŸ† Acknowledgments
- *MediaPipe* for real-time hand tracking.
- *TensorFlow/Keras* for training the neural network.
- *Flask & React* for smooth integration of backend and frontend.

ğŸ¯ *This project showcases the power of AI and computer vision in breaking communication barriers!* ğŸš€
